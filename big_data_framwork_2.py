# -*- coding: utf-8 -*-
"""big data framwork 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EEM-2SzbFca-Dzx2-d66N3uzZ89BE79j
"""

# Install pyspark
!pip install pyspark

"""## Step 1: Reading and Understanding the Data

Let's start with the following steps:

1. Importing data using the pandas library
2. Understanding the structure of the data
"""

'''pyspark'''
from pyspark import SparkContext
from pyspark import SparkFiles

'''to load the csv from an url to a rdd'''
import urllib.request

'''to convert string to date'''
from datetime import datetime

'''for splitting and normalization'''
import random

'''to get statistical informations'''
import statistics

#i created a public repository on github to store the datasets i will use this year to make them easier to access

#creating an url variable because we can't load directly a csv from an url to a rdd
url = "https://raw.githubusercontent.com/Shifuumi/Time-Series-analysis/main/london_merged.csv"
response = urllib.request.urlopen(url)

# Create a SparkContext object or get one if already existing
# Download the CSV file and create an RDD from it
rdd = SparkContext.getOrCreate().parallelize(response.read().decode('utf-8').splitlines())

print(rdd.collect())

#equivalent to the df.shape
#get the number of rows in the rdd and the number of partitions
print(rdd.count())
print(rdd.getNumPartitions())

#creating empty list
double_tab = []

#splitting each element of the rdd by ',' and adding the result to the double_tab list
for elem in rdd.collect():
    double_tab.append(elem.split(","))

#equivalent of df.head()
for i in range(0, 10):
  print(double_tab[i])

#the first list in double_tab contains the name of each 'column'

#get the name of each 'column'
name = rdd.first()

#get all of the elements that ARE NOT the name
rows_rdd = rdd.filter(lambda x: x != name)

#stocking the values of rows_rdd in a new variable rows to avoid using .collect() every time
rows = rows_rdd.collect()

#splitting the content of name by ',' to get every column names separately
name = (rdd.first()).split(',')

#printing the elements we have like df.head()
for j in name: print(j, end=" | ")
print("\n")
for i in range(0, 9):
  print("Rows", i, ":", rows[i])

#######################utilitaires#################################
def strptime(date_string, format):
    month_dict = {
        '01': 1, '02': 2, '03': 3, '04': 4,
        '05': 5, '06': 6, '07': 7, '08': 8,
        '09': 9, '10': 10, '11': 11, '12': 12
    }

    year = int(date_string[0:4])
    month = month_dict[date_string[5:7]]
    day = int(date_string[8:10])
    hour = int(date_string[11:13])
    minute = int(date_string[14:16])
    second = int(date_string[17:19])

    return (year, month, day, hour, minute, second)

#creating a bike list and adding every element of rows in the right type and in the same order as the original project

bike = []

#for every elements in the rows list
for row in rows: 

    #creating a temporary list which will be added to the bike list 
    elem = [] 

    #splitting the row by ','
    row = row.split(",")

    #casting the 1st elment from string to datetime with the specified format
    date = datetime.strptime(row[0], "%Y-%m-%d %H:%M:%S")

    #adding the new date to the elem list
    elem.append(date)

    #casting the 2nd to the 5th elments from string to float 
    floats = [float(x) for x in row[1:6]]

    #adding the floats to the elem list
    elem.extend(floats)
    
    #casting the 7th and 10th elements of the row from string to int to row (string to float directly is not working in that case)
    #and adding them to the elem list
    elem.append(int(float(row[6])))
    elem.append(int(float(row[9])))

    #casting the 8th to the 9th elments from string to boolean 
    bools = [bool(float(x)) for x in row[7:9]]

    #adding the booleans to the elem list
    elem.extend(bools)

    #adding the elem list to the bike list
    bike.append(elem)

#creating a new name list with desired name and order
name = ['date', 'count', 'temperature in C', 'temperature feels like in c', 'humidity', 'wind speed', 'weather', 'season','is_holiday', 'is_weekend']

#printing the elements we have like df.head()
for j in name: print(j, end=" | ")
print("\n")
for i in range(0, 9):
  print("Rows", i, ":", rows[i])

"""## Step 4: Splitting the Data into Training and Testing Sets

Before model building, you first need to perform the test-train split and scale the features.
"""

#creating the function to split the data
#original function is train_test_split from sklearn.model_selection library

'''
This function should : 
  - take a double list and split it in two lists : train_data and test_data
  - take a 2nd argument which is the test_size, which is the fraction of the dataset to be used as test data
  - take a 3rd argument which controls the random seed for reproducibility
'''
def train_test_split(data, test_size, random_state):
    #get the size of our set
    size = len(data)

    #get the size of the final test_data set
    size_test = int(size * test_size)

    #get the size of the final train_data set
    size_train = size - size_test

    #if the random_state arg is significant, we use it to create the seed used to pick randomly the data to split
    # this random pick can be repeated if we use the same random_state
    if random_state is not None:
        random.seed(random_state)
    
    #creating a list containing an index for every element in data
    ind = list(range(size))

    #shuffle the ind list using the seed
    random.shuffle(ind)

    #rearranging the order of the elements in the data list to correspond to the shuffled ind list
    data = [data[i] for i in ind]

    #creating the two final lists with the new ordered data list
    train_data, test_data = data[:size_train], data[size_train:]

    #returning the splitted lists
    return test_data, train_data

#using the function created before to split our bike list into 2 subsets : bike_train and bike_test
bike_train, bike_test = train_test_split(bike, 0.7, 100)

#printing the size of each new subsets
print(len(bike_train))
print(len(bike_test))

"""### Rescaling the Features

It is important to have all the variables on the same scale for the model to be easily interpretable. 
We can use standardization or normalization so that the units of the coefficients obtained are all on the same scale. 
- **There are two common ways of rescaling:**

- Min-Max scaling (Normalisation):Between 0 and 1
- Standardisation :mean-0, sigma-1

##### Min-Max scaling
"""

#creating the function to normalize the data
#original function is train_test_split from sklearn.preprocessing library
# all of our values should be between 0 and 1 included

'''
This function should : 
  - determine the minimum and the maximum value for each variable
  - substract the minimum value to each values of a variable
  - divide each values of a variable by the difference between the max and the min value

'''

def normalize(data):
    # Searching for min and max for each variables

    #creating a list containing the index of the variables we want to scale
    num_vars = [2, 4, 5, 1]

    #creating 2 lists containg 12x (nb total of variables) a min value and max value that cannot be reach in the dataset
    mins = [99999] * 12
    maxs = [-99999] * 12

    #for every element in our set
    for row in data:

      #searching for specific variables in num_vars
        for i in num_vars:

            #if the value of row[i] is lesser then the value of mins[i], we replace mins[i] by the value of row[i]
            if row[i] < mins[i]:
                mins[i] = row[i]
            
            #if the value of row[i] is greater then the value of maxs[i], we replace maxs[i] by the value of row[i]
            if row[i] > maxs[i]:
                maxs[i] = row[i]
    
    # normalization
    for row in data:

        #for every values in the specified index
        for i in num_vars:

            #applying the formula to normalize
            row[i] = (row[i] - mins[i]) / (maxs[i] - mins[i])
  
    #returning the normalized data
    return data

#Checking numeric variables(min and max) before scaling

#creating a list for every variables we want statisticals values
count = []
t1 = []
t2 = []
humidity = []
wind = []

#adding every values to the correct list from above
for x in bike_train:
  count.append(x[1])
  t1.append(x[2])
  t2.append(x[3])
  humidity.append(x[4])
  wind.append(x[5])

#printing statisticals informations with the statistics library
print("COUNT: \nMean:", statistics.mean(count), '\n', "Standard deviation:", statistics.stdev(count), '\n', "Minimum:", min(count), '\n', "Maximum:", max(count))
print("\nTEMPERATURE: \nMean:", statistics.mean(t1), '\n', "Standard deviation:", statistics.stdev(t1), '\n', "Minimum:", min(t1), '\n', "Maximum:", max(t1))
print("\nFEELS LIKE: \nMean:", statistics.mean(t2), '\n', "Standard deviation:", statistics.stdev(t2), '\n', "Minimum:", min(t2), '\n', "Maximum:", max(t2))
print("\nHUMIDITY: \nMean:", statistics.mean(humidity), '\n', "Standard deviation:", statistics.stdev(humidity), '\n', "Minimum:", min(humidity), '\n', "Maximum:", max(humidity))
print("\nWIND SPEED: \nMean:", statistics.mean(wind), '\n', "Standard deviation:", statistics.stdev(wind), '\n', "Minimum:", min(wind), '\n', "Maximum:", max(wind))

#printing the first row of bike_train to see the data
print(bike_train[0])

#normalizing the bike_train subset
bike_train = normalize(bike_train)


#printing the first row of bike_train to see if it is normalized
print(bike_train[0])

#Checking numeric variables(min and max) after scaling
#creating a list for every variables we want statisticals values
count = []
t1 = []
t2 = []
humidity = []
wind = []

#adding every values to the correct list from above
for x in bike_train:
  count.append(x[1])
  t1.append(x[2])
  t2.append(x[3])
  humidity.append(x[4])
  wind.append(x[5])

#printing statisticals informations with the statistics library
print("COUNT: \nMean:", statistics.mean(count), '\n', "Standard deviation:", statistics.stdev(count), '\n', "Minimum:", min(count), '\n', "Maximum:", max(count))
print("\nTEMPERATURE: \nMean:", statistics.mean(t1), '\n', "Standard deviation:", statistics.stdev(t1), '\n', "Minimum:", min(t1), '\n', "Maximum:", max(t1))
print("\nFEELS LIKE: \nMean:", statistics.mean(t2), '\n', "Standard deviation:", statistics.stdev(t2), '\n', "Minimum:", min(t2), '\n', "Maximum:", max(t2))
print("\nHUMIDITY: \nMean:", statistics.mean(humidity), '\n', "Standard deviation:", statistics.stdev(humidity), '\n', "Minimum:", min(humidity), '\n', "Maximum:", max(humidity))
print("\nWIND SPEED: \nMean:", statistics.mean(wind), '\n', "Standard deviation:", statistics.stdev(wind), '\n', "Minimum:", min(wind), '\n', "Maximum:", max(wind))

"""#### Dividing into X and Y sets for the model building"""

#Divide the data into X and y

#y_train should contains every 'count' values 

#creating the y_train list
y_train = []

#adding every 'count' values in y_train
for x in bike_train:
  y_train.append(x[1])


#x_train should contains every other values 

#creating the x_train list
x_train = []

#adding the other values in x_train
for y in bike_train:
  x_train.append([y[1], y[2], y[3], y[4], y[5]])

print(x_train[0])
print(y_train[0])